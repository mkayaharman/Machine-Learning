{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UCsvzbImqZN"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0Iul5QymqZN"
   },
   "source": [
    "# #Determines\n",
    "\n",
    "**Auto Scout** data which using for this project, scraped from the on-line car trading company(https://www.autoscout24.com)in 2019, contains many features of 9 different car models. In this project, you will use the data set which is already preprocessed and prepared for algorithms .\n",
    "\n",
    "The aim of this project to understand of machine learning algorithms. Therefore, you will not need any EDA process as you will be working on the edited data.\n",
    "\n",
    "---\n",
    "\n",
    "In this Senario, you will estimate the prices of cars using regression algorithms.\n",
    "\n",
    "While starting you should import the necessary modules and load the data given as pkl file. Also you'll need to do a few pre-processing before moving to modelling. After that you will implement ***Linear Regression, Ridge Regression, Lasso Regression,and Elastic-Net algorithms respectively*** (After completion of Unsupervised Learning section, you can also add bagging and boosting algorithms such as ***Random Forest and XG Boost*** this notebook to develop the project. You can measure the success of your models with regression evaluation metrics as well as with cross validation method.\n",
    "\n",
    "For the better results, you should try to increase the success of your models by performing hyperparameter tuning. Determine feature importances for the model. You can set your model with the most important features for resource saving. You should try to apply this especially in Random Forest and XG Boost algorithms. Unlike the others, you will perform hyperparameter tuning for Random Forest and XG Boost using the ***GridSearchCV*** method. \n",
    "\n",
    "Finally You can compare the performances of algorithms, work more on the algorithm have the most successful prediction rate.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL-jJ1irmqZN"
   },
   "source": [
    "# #Tasks\n",
    "\n",
    "#### 1. Import Modules, Load Data and Data Review\n",
    "#### 2. Data Pre-Processing\n",
    "#### 3. Implement Linear Regression \n",
    "#### 4. Implement Ridge Regression\n",
    "#### 5. Implement Lasso Regression \n",
    "#### 6. Implement Elastic-Net\n",
    "#### 7. Visually Compare Models Performance In a Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rik6d_vimqZN"
   },
   "source": [
    "## 1. Import Modules, Load Data and Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBddHlqgmqZN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd      \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_scout_not_dummy2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object = df.select_dtypes(include=\"object\").head()\n",
    "df_object\n",
    "\n",
    "# select_dtypes(include=\"object\") to only include the columns of object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_object:\n",
    "    print(f\"{col:<30}:\", df[col].nunique())\n",
    "\n",
    "# <30 here sets the character length before the column (:) so that all of the outputs are aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Extras feature from object to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.Extras:\n",
    "    print(i)\n",
    "\n",
    "# We want to preprocess this feature, as from our domain knowledge, the number of extras a car has plays an important role in its price.\n",
    "# Keep in mind that this variable does not consist of atomic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.Extras:\n",
    "    print(len(i.split(\",\")))\n",
    "\n",
    "# print out the number of extras in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Extras.apply(lambda x: len(x.split(',')))\n",
    "\n",
    "# We can also do it like this. Remember that whenever you find yourself using a loop over a dataframe, you're probably making a mistake as\n",
    "# there will almost always a better way of doing it in pandas than looping the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Extras\"] = df.Extras.apply(lambda x: len(x.split(',')))\n",
    "\n",
    "# Replacing Extras with the number of extras each record has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()  # confirm the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Extras.nunique()  # Got to 10 unique values from 659. Huge reduction in cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Extras.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.select_dtypes(\"object\"):\n",
    "    print(f\"{i:<30}:\", df[i].unique())\n",
    "    \n",
    "# We can see that the first 8 features should be encoded with onehotencoder, whereas the last 3 ones with ordinalencoder.\n",
    "# OneHotEncoder:\n",
    "# It encodes nominal categorical variables as dummy variables. We use this in case of nominal variables to avoid hinting our model that\n",
    "# there's an underlying logical order to our values when in reality that's not the case.\n",
    "\n",
    "# OrdinalEncoder:\n",
    "# If there's a logical order to your values (good, bad, worse) (warm, hot, hottest), we will use\n",
    "# OrdinalEncoder to encode our variables numerically such that we preserve their logical order.\n",
    "# See statistical variable types for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.make_model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install \"matplotlib>=3.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = df.make_model.value_counts().plot(kind =\"bar\")\n",
    "\n",
    "ax.bar_label(ax.containers[0]);\n",
    "\n",
    "# Axis object has a bar_label method to annotate the bar heights.\n",
    "# Make sure that your matplotlib version is >= 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.price, bins=50, kde=True);\n",
    "\n",
    "# We are checking the distribution of our target to see if we have any outliers, since linear\n",
    "# models are highly susceptible to outliers. In the context of ML, outliers don't necessarily\n",
    "# indicate an underlying problem in the data gathering process. It means that we don't have\n",
    "# enough records that represent a certain area in the target which is called \"underrepresented\".\n",
    "# And the model will most likely not learn enough about these underrepresented areas.\n",
    "\n",
    "# In the histogram below, we can see that above 40k EUR, we dont have enough observation\n",
    "# points to represent that longer right tail.\n",
    "\n",
    "# Train one model without the outliers and train one with the outliers to see which one works better.\n",
    "\n",
    "# It also might be a better idea to group the observations by their car make to check if they have outliers within their price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=\"number\")\n",
    "df_numeric\n",
    "\n",
    "# select_dtypes(include=\"number\") to get only the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_numeric.corr(), annot=True, vmin=-1, vmax=1, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multicollinearity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.corr()[(df_numeric.corr()>= 0.9) & (df_numeric.corr() < 1)].any().any()\n",
    "\n",
    "# Check if any 2 features have a correlation between 0.9 and 1 which indicates high positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.corr()[(df_numeric.corr()<= -0.9) & (df_numeric.corr() > -1)].any().any()\n",
    "\n",
    "# Check if any 2 features have a correlation between -0.9 and -1 which indicates high negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.corr()[(abs(df_numeric.corr())>= 0.9) & (abs(df_numeric.corr()) < 1)].any().any()\n",
    "\n",
    "# Combine them both into a one-liner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers in Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.subplot(211)\n",
    "sns.boxplot(df.price)\n",
    "\n",
    "plt.subplot(212)\n",
    "sns.stripplot(df.price);\n",
    "\n",
    "# Can check both boxplot and stripplot to see if we have any outliers and where they start from. If you have too many outliers (too long of a tail on either side), you can play\n",
    "# with the whisker distance to accommodate some of the so-called outliers if you don't want to lose too many records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(211)\n",
    "sns.boxplot(x=\"make_model\", y=\"price\", data=df, whis=1.5)\n",
    "\n",
    "plt.subplot(212)\n",
    "sns.boxplot(x=\"make_model\", y=\"price\", data=df, whis=1.5)\n",
    "sns.stripplot(x=\"make_model\", y=\"price\", data=df);\n",
    "\n",
    "# We can set individual whisker values per group, as they have different outlier situations. It's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.make_model.unique()\n",
    "\n",
    "# Unique valies in make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"make_model\"] == \"Audi A1\"][\"price\"]\n",
    "\n",
    "# The price values for Audi A1 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_outliers = []\n",
    "\n",
    "for model in df.make_model.unique():\n",
    "    \n",
    "    car_prices = df[df[\"make_model\"] == model][\"price\"]\n",
    "    \n",
    "    Q1 = car_prices.quantile(0.25)\n",
    "    Q3 = car_prices.quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    lower_lim = Q1 - 1.5 * IQR\n",
    "    upper_lim = Q3 + 1.5 * IQR\n",
    "    \n",
    "    count_of_outliers = (car_prices[(car_prices < lower_lim) | (car_prices > upper_lim)]).count()\n",
    "    \n",
    "    total_outliers.append(count_of_outliers)\n",
    "    \n",
    "    print(f\" The count of outlier for {model:<15} : {count_of_outliers:<5}, \\\n",
    "          The rate of outliers : {(count_of_outliers/len(df[df['make_model']== model])).round(3)}\")\n",
    "print()    \n",
    "print(\"Total_outliers : \", sum(total_outliers), \"The rate of total outliers :\", (sum(total_outliers)/len(df)).round(3))\n",
    "\n",
    "\n",
    "# Getting potential outliers per make_model based on a 1.5 whisker range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing\n",
    "\n",
    "As you know, the data set must be processed before proceeding to the implementation of the model. As the last step before model fitting, you need to split the data set into train and test. Then, you should train the model with the training data and evaluate the performance of the model on the test data. You can use the train and test data you have created for all algorithms.\n",
    "\n",
    "You must also drop your target variable, the column you are trying to predict.\n",
    "\n",
    "You can use many [performance metrics for regression](https://medium.com/analytics-vidhya/evaluation-metrics-for-regression-problems-343c4923d922) to measure the performance of the regression model you train. You can define a function to view different metric results together.\n",
    "\n",
    "You can also use the [cross validation](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85) method to measure the estimator performance. Cross validation uses different data samples from your test set and calculates the accuracy score for each data sample. You can calculate the final performance of your estimator by averaging these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train | Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"price\")\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "\n",
    "# OneHotEncoder will encode your nominal categorical variables as binary dummy variables which is fundamentally important to avoid\n",
    "# hinting the model that there is an underlying logical order to the nominal categories. We don't use get_dummies() function\n",
    "# for this purpose because pandas functions are not well suited for scikit-learn methodology (creating different parts from the\n",
    "# dataset such as train/test), since pandas was not made with machine learning in mind. It's a data analysis library.\n",
    "\n",
    "# As always, we will fit the encoder on the train set, and transform both the train and test sets using what the encoder\n",
    "# will have learnt from the train set. handle_unknown=\"ignore\" is essential here, in case of a situation where\n",
    "# your test set has a categorical value that was not seen by the encoder in the train set. It will handle\n",
    "# this expception by \"ignoring\" it. The default value for this parameter is \"error\" which will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {\"train\":['good','bad','worst','good', 'good', 'bad', 'bed']}\n",
    "test = {\"test\": ['bad','worst','good', 'good', 'bad', \"bed\", \"resume\", \"car\"]}\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "enc.fit_transform(train[[\"train\"]])\n",
    "\n",
    "# The fitting is done ONLY on the train set. As always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "enc.transform(test[[\"test\"]])\n",
    "\n",
    "\n",
    "# And then we transform the test set with the unique values learnt from the train set.\n",
    "# Notice here that the test set had 2 values that were not seen in the train set, yet the encoder\n",
    "# handles this exception gracefully by \"ignoring\" those values (putting all 0 to those observation points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.get_feature_names_out([\"train\"])\n",
    "\n",
    "# OneHotEncoder will encode the nominal categorical variables as dummy variables, which will\n",
    "# add the same new dummy/binary variables as the number of unique values in each categorical variable being encoded.\n",
    "# And it will drop the original categorical variable afterwards. The newly added dummy feature names will follow\n",
    "# the convention of <the name of the old categorical feature>_<name of the categorical value>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(enc.fit_transform(train[[\"train\"]]), columns = enc.get_feature_names_out([\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(enc.transform(test[[\"test\"]]), columns = enc.get_feature_names_out([\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = {\"train\":['good','bad','worst','good', 'good', 'bad']}\n",
    "test2 = {\"test\": ['bad','worst','good', 'good', 'bad']}\n",
    "train2 = pd.DataFrame(train2)\n",
    "test2 = pd.DataFrame(test2)\n",
    "train2\n",
    "\n",
    "# If there's an underlying logical order to a categorical variable (which is called ordinal variable), we need to handle it\n",
    "# accordingly by using OrdinalEncoder in order to avoid losing the logical order.\n",
    "\n",
    "# There's an exception to this approach: In tree based models, we will encode all categorical variables with onehotencoder\n",
    "# regardless of the difference between nominal and ordinal variables. We will talk about that more in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categories = ['worst','bad','good']\n",
    "\n",
    "enc_2 = OrdinalEncoder(categories=[categories])\n",
    "\n",
    "# If there's a logical order to the categorical values, we will use ordinalencoder. One important note here is that\n",
    "# by default, ordinalencoder will order the categorical values in their alphabetical order. So, by default,\n",
    "# it will encode bad:0, good:1, worst:2 which is not the correct logical order they should be.\n",
    "# This is why we are specifying the categories variable in the exact logical order that they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "enc_2.fit_transform(train2[[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "enc_2.transform(test2[[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_2.get_feature_names_out([\"train\"])\n",
    "\n",
    "# OrdinalEncoder doesn't change the feature name. It just encodes the values of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder  and OrdinalEncoder for X_train\n",
    "\n",
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.select_dtypes(\"object\"):\n",
    "    print(f\"{i:<30}:\", df[i].unique())\n",
    "\n",
    "# First 8 features should be encoded with onehotencoder as they dont have logical orders.\n",
    "# The last 3 features should be encoded with ordinalencoder as they have a logical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = X_train.select_dtypes(\"object\").columns\n",
    "cat\n",
    "\n",
    "# Names of the features that need to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_onehot = ['make_model', 'body_type', 'Type', 'Fuel', 'Paint_Type','Upholstery_type', 'Gearing_Type', 'Drive_chain']\n",
    "cat_ordinal = ['Comfort_Convenience_Package', 'Entertainment_Media_Package', 'Safety_Security_Package']\n",
    "\n",
    "# Creating 2 separate lists. One for variables that will be onehotencoded, one for variables that will be ordinalencoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cat_onehot].head()\n",
    "\n",
    "# X_train features that will be onehotencoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "enc.fit_transform(X_train[cat_onehot])\n",
    "\n",
    "# Did the onehotencoder transformation to the relevant features of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.get_feature_names_out(cat_onehot)\n",
    "\n",
    "# These are the new feature names that we got after doing the OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = pd.DataFrame(enc.fit_transform(X_train[cat_onehot]), index=X_train.index, \n",
    "                           columns=enc.get_feature_names_out(cat_onehot))\n",
    "X_train_onehot\n",
    "\n",
    "# Transformers in scikit-learn ALWAYS return np.ndarray objects. In order for us to use them as pandas objects, we have to\n",
    "# turn them into dataframes. We can get the new names from the encoder itself directly by saying enc.get_feature_names_out(cat_onehot).\n",
    "# Also, when we turn them into dataframes, we will lose the index information. We retrieve the original index information from the old X_train.\n",
    "# This is very important to keep track of our records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ordinal = ['Comfort_Convenience_Package', 'Entertainment_Media_Package', 'Safety_Security_Package']\n",
    "\n",
    "# Features to OrdinalEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_ordinal:\n",
    "    print(f\"{i:<27}:\", df[i].unique())\n",
    "\n",
    "# Check their unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_for_comfort = ['Standard', 'Premium', 'Premium Plus']\n",
    "cat_for_ent = ['Standard', 'Plus']\n",
    "cat_for_safety = ['Safety Standard Package', 'Safety Premium Package', 'Safety Premium Plus Package']\n",
    "\n",
    "enc2 = OrdinalEncoder(categories=[cat_for_comfort, cat_for_ent, cat_for_safety])\n",
    "\n",
    "# Manually arrange the unique category names in the correct logical order.\n",
    "\n",
    "# Also, make sure that the order of the \"categories\" variable values are the same as the order\n",
    "# of the columns that you want to OrdinalEncode like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cat_ordinal]\n",
    "\n",
    "# Getting the features to OrdinalEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2.fit_transform(X_train[cat_ordinal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2.get_feature_names_out(cat_ordinal)\n",
    "\n",
    "# Feature names are the same, as OrdinalEncoder doesn't change the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ordinal = pd.DataFrame(enc2.fit_transform(X_train[cat_ordinal]), index = X_train.index, \n",
    "                           columns = enc2.get_feature_names_out(cat_ordinal))\n",
    "\n",
    "X_train_ordinal\n",
    "\n",
    "# turn it back into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining All Features of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train.select_dtypes(\"number\")\n",
    "X_train_numeric.head()\n",
    "\n",
    "# We are getting the numerical features of our X_train which we did not encode in any way.\n",
    "# We will combine all of them back into a one big dataframe (numeric_df + ordinal_df + onehot_df)\n",
    "\n",
    "# Since we retained the original index values of our data points after we did OrdinalEncoder and OneHotEncoder, their indexes\n",
    "# are the same now with the indexes of X_train_numeric, which makes it very easy to join them back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train_numeric.join([X_train_onehot, X_train_ordinal])\n",
    "X_train_new\n",
    "\n",
    "# Joining back them together on the same index numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder  and OrdinalEncoder for X_test\n",
    "\n",
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_onehot = pd.DataFrame(enc.transform(X_test[cat_onehot]), index = X_test.index, \n",
    "                             columns = enc.get_feature_names_out(cat_onehot))\n",
    "X_test_onehot\n",
    "\n",
    "# We will do the same process on the test set as well. One important difference, though, is that\n",
    "# we ONLY TRANSFORM the test set. NOT FIT it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ordinal = pd.DataFrame(enc2.transform(X_test[cat_ordinal]), index = X_test.index, \n",
    "                           columns = enc2.get_feature_names_out(cat_ordinal))\n",
    "\n",
    "X_test_ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining All Features of X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numeric = X_test.select_dtypes(\"number\")\n",
    "X_test_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = X_test_numeric.join([X_test_onehot, X_test_ordinal])\n",
    "X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Object Features into Numerical Features Using Make Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_onehot = ['make_model', 'body_type', 'Type', 'Fuel', 'Paint_Type','Upholstery_type', 'Gearing_Type', 'Drive_chain']\n",
    "cat_ordinal = ['Comfort_Convenience_Package', 'Entertainment_Media_Package', 'Safety_Security_Package']\n",
    "    \n",
    "cat_for_comfort = ['Standard', 'Premium', 'Premium Plus']\n",
    "cat_for_ent = ['Standard', 'Plus']\n",
    "cat_for_safety = ['Safety Standard Package', 'Safety Premium Package', 'Safety Premium Plus Package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "column_trans = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_onehot), \n",
    "                                       (OrdinalEncoder(categories= [cat_for_comfort, cat_for_ent, cat_for_safety]), cat_ordinal),\n",
    "                                       remainder='passthrough') # MinMaxScaler()\n",
    "\n",
    "\n",
    "# As you can see, this process is actually extremely long and cumbersome. There's a saviour, though. \n",
    "# ColumnTransformer from scikit-learn was created exactly for this purpose. To automate all this long and tedious process.\n",
    "# Because doing this process manually is very error prone. By automating it, things get much more reliable and consistent \n",
    "# and error proof.\n",
    "\n",
    "# It will automate the processing of features in the order that is specified when we instantiate the calss.\n",
    "\n",
    "# remainder=\"passthrough\" allows us to passthrough the remainder of the columns that we are not doing anything with.\n",
    "# The default value for remainder is \"drop\" which will drop all of the remainder columns.\n",
    "# You can also do some other stuff with this parameter as well such as remainder=MinMaxScaler() which will\n",
    "# scale the remaining columns. It in fact accepts any transformer that is implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = column_trans.fit_transform(X_train)\n",
    "X_test_trans = column_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans.shape, X_test_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = column_trans.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= pd.DataFrame(X_train_trans, columns=features, index=X_train.index)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= pd.DataFrame(X_test_trans, columns=features, index=X_test.index)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.join(y_train).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_by_price = X_train.join(y_train).corr()[\"price\"].sort_values()[:-1]\n",
    "corr_by_price\n",
    "\n",
    "# We want to check the correlation of all of my independent features with my target feature. This is why\n",
    "# we are joining the target with the independent features here. We obviously know the correlation of \n",
    "# the feature with itself is 1 so we are excluding it with [:-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,14))\n",
    "sns.barplot(y = corr_by_price.index, x = corr_by_price)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout();\n",
    "\n",
    "# Visualise the correlations to make it more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An_5ITcAmqZO"
   },
   "source": [
    "## Implement Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c0y-w8vmqZO"
   },
   "source": [
    " - Import the module\n",
    " - Fit the model \n",
    " - Predict the test set\n",
    " - Determine feature coefficients\n",
    " - Evaluate model performance (use performance metrics for regression and cross_val_score)\n",
    " - Compare different evaluation metrics\n",
    " \n",
    "*Note: You can use the [dir()](https://www.geeksforgeeks.org/python-dir-function/) function to see the methods you need.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq6UKn_HmqZO"
   },
   "outputs": [],
   "source": [
    "def train_val(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    scores = {\"train\": {\"R2\" : r2_score(y_train, y_train_pred),\n",
    "    \"mae\" : mean_absolute_error(y_train, y_train_pred),\n",
    "    \"mse\" : mean_squared_error(y_train, y_train_pred),                          \n",
    "    \"rmse\" : np.sqrt(mean_squared_error(y_train, y_train_pred))},\n",
    "    \n",
    "    \"test\": {\"R2\" : r2_score(y_test, y_pred),\n",
    "    \"mae\" : mean_absolute_error(y_test, y_pred),\n",
    "    \"mse\" : mean_squared_error(y_test, y_pred),\n",
    "    \"rmse\" : np.sqrt(mean_squared_error(y_test, y_pred))}}\n",
    "    \n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "# We will use this function to compare train and test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Instantiate the linear model into an object and train it on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# This way, pandas will show us 3 floating points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_val(lm, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_r2(y_test, y_pred, X):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    n = X.shape[0]   # number of observations\n",
    "    p = X.shape[1]   # number of independent variables \n",
    "    adj_r2 = 1 - (1-r2)*(n-1)/(n-p-1)\n",
    "    return adj_r2\n",
    "\n",
    "#There are too many featuras in our data but not enough rows, this is a false improvement in our model's R2_score.\n",
    "#Especially in such data or if too many new features (such as dummies feature) have been added to our data.\n",
    "#We need to detect real R2_score with adjusted R2_score.\n",
    "\n",
    "\n",
    "# Adjusted R2_score checks the trade off between row and feature count and returns us a score. If numbers\n",
    "#There will be serious decreases in adjusted_R2_score if there is a large imbalance between Above for Adjusted R2 Score\n",
    "#We define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r2(y_test, y_pred, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "scores = cross_validate(model, X_train_scaled, y_train, scoring=['r2', \n",
    "            'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], cv =10,\n",
    "             return_train_score=True)\n",
    "\n",
    "# As we learned in our previous lessons, we get the overfiting control through cross validaition.\n",
    "# We do this by comparing train and validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).iloc[:, 2:].mean()\n",
    "\n",
    "# train ve validaiton scores close. So no Overfiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(lm, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2622/df.price.mean()\n",
    "\n",
    "# models average error  %14.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "from yellowbrick.features import RadViz\n",
    "\n",
    "visualizer = RadViz(size=(720, 3000))\n",
    "model = LinearRegression()\n",
    "visualizer = PredictionError(model)\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();\n",
    "\n",
    "# With the prediciton error image, we can see how well the predictions made by our model are. When we look at it, we can see that the cars priced at 40 thousand EURO and above are pulling our best fit line down.\n",
    "# If I have determined from the data on the basis of cars or models of 40 thousand EURO and above that I have seen spoil my scores.\n",
    "# I can get better scores when I remove outlier priced cars from my data and retrain the model from this data.\n",
    "\n",
    "# In this notebook, we will continue by removing the outlier values from our data. but not 40 thousand EURO cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plot with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "visualizer = RadViz(size=(1000, 720))\n",
    "model = LinearRegression()\n",
    "visualizer = ResidualsPlot(model)\n",
    "\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping outliers that worsen my predictions from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in df2.make_model.unique():\n",
    "    \n",
    "    car_prices = df2[df2[\"make_model\"]== model][\"price\"]\n",
    "    \n",
    "    Q1 = car_prices.quantile(0.25)\n",
    "    Q3 = car_prices.quantile(0.75)\n",
    "    \n",
    "    IQR = Q3-Q1\n",
    "    \n",
    "    lower_lim = Q1-1.5*IQR\n",
    "    upper_lim = Q3+1.5*IQR\n",
    "\n",
    "    drop_index = df2[df2[\"make_model\"]== model][(car_prices < lower_lim) | (car_prices > upper_lim)].index\n",
    "    df2.drop(index = drop_index, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "df2\n",
    "\n",
    "# Here we extract outlier observations from our data. First, determine the lower and upper limits and stay outside these limits.\n",
    "# We determine the indexes of the remaining car prices and drop these indexes from our data.\n",
    "# We use reset_index to ignore the indexes we drop and make the index order properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15493 + 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "\n",
    "# df3 is new dataset cleaned from outliers. I keep it maybe i can use it laters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns = \"price\")\n",
    "y = df2.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= pd.DataFrame(column_trans.fit_transform(X_train), columns=features, index=X_train.index)\n",
    "X_test= pd.DataFrame(column_trans.transform(X_test), columns=features, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "lm2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = RadViz(size=(720, 3000))\n",
    "model = LinearRegression()\n",
    "visualizer = PredictionError(model)\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();\n",
    "\n",
    "# After removing the outlier values, we see that the angle between the best fit line and the identity line narrows even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plot without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "visualizer = RadViz(size=(1000, 720))\n",
    "model = LinearRegression()\n",
    "visualizer = ResidualsPlot(model)\n",
    "\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(lm2, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# After the outliers are out, we can see that the results are getting better."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eval metrics with outliers\n",
    "\n",
    "        train\t     test\n",
    "R2\t    0.868\t     0.871\n",
    "mae\t    1857.146\t 1858.159\n",
    "mse\t    7229219.239\t 6877350.088\n",
    "rmse\t2688.721\t 2622.470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2256/df2.price.mean()\n",
    "\n",
    "# without outliers avreage prediction error decreased from %14.55 to %12,83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2622/df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #normalize=True\n",
    "scores = cross_validate(model, X_train_scaled, y_train,\n",
    "                        scoring=['r2', 'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], \n",
    "                        cv=10, return_train_score=True)\n",
    "\n",
    "#overfitting check with new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:].mean()\n",
    "\n",
    "# train and validation skoces close so no overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(lm2, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Since the test scores and the validation scores we got from the CV are close to each other, \n",
    "# We can say that the scores we got from the test (hold out) set are consistent scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm2.predict(X_test_scaled)\n",
    "\n",
    "lm_R2 = r2_score(y_test, y_pred)\n",
    "lm_mae = mean_absolute_error(y_test, y_pred)\n",
    "lm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# To compare the scores we get from the linear model, we assign the scores to the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.coef_  # oThe coefficients of the features with onehot encoder applied are very high. Dummy variable trap\n",
    "\n",
    "# https://geoffruddock.com/one-hot-encoding-plus-linear-regression-equals-multi-collinearity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lm2.coef_, index = X_train.columns, columns=[\"Coef\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns = [\"price\"])\n",
    "y = df2.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# After dropping outlier observations, we divide the remaining data into X and y again and divide it into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What happens can be described as follows:\n",
    "\n",
    "The data are split into TRAINING data and TEST data according to ratio of train_test_split\n",
    "\n",
    "fit process:\n",
    "Step 1: the onehotencoder is fitted on the categoric features in cat_onehot list for TRAINING data\n",
    "Step 2: the onehotencoder transforms the categoric features in cat_onehot list for TRAINING data\n",
    "step 3: the ordinalencoder is fitted on the categoric features in cat_ordinal list for TRAINING data\n",
    "Step 4: the ordinalencoder transforms the categoric features in cat_ordinal list for TRAINING data\n",
    "step 5: the numeric features of TRAINING data are left as they are\n",
    "Step 6: the minmaxscaler is fitted on all features of transformed TRAINING data\n",
    "Step 7: the minmaxscaler transforms all features of transformed TRAINING data\n",
    "Step 8: the models are fitted/trained using the scaled and transformed TRAINING data\n",
    "\n",
    "predict process:\n",
    "Step 1: the onehotencoder transforms the categoric features in cat_onehot list of TEST data according to TRAINING data\n",
    "Step 2: the ordinalencoder transforms the categoric features in cat_ordinal list of TEST data according to TRAINING data\n",
    "step 3: the numeric features of TRAINING data are left as they are\n",
    "Step 4: the minmaxscaler transforms all features of TEST data according to TRAINING data\n",
    "Step 5: the trained models predict using the scaled and transformed TEST data\n",
    "\n",
    "\n",
    "pipe_model.fit(X_train, y_train) --> columns_trans.fit_transform(X_train) --> Lasso.fit(X_train_trans, y_train)\n",
    "pipe_model.predict(X_test)       --> columns_trans.transform(X_test)      --> Lasso.predict(X_test_trans)\n",
    "\n",
    "Pipeline burda açıklandığı gibi işlemlerimizi otomotize eder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_onehot = ['make_model', 'body_type', 'Type', 'Fuel', 'Paint_Type','Upholstery_type', 'Gearing_Type', 'Drive_chain']\n",
    "cat_ordinal = ['Comfort_Convenience_Package', 'Entertainment_Media_Package', 'Safety_Security_Package']\n",
    "    \n",
    "cat_for_comfort = ['Standard', 'Premium', 'Premium Plus']\n",
    "cat_for_ent = ['Standard', 'Plus']\n",
    "cat_for_safety = ['Safety Standard Package', 'Safety Premium Package', 'Safety Premium Plus Package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "enc_onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "enc_ordinal = OrdinalEncoder(categories= [cat_for_comfort, cat_for_ent, cat_for_safety])\n",
    "\n",
    "column_trans = make_column_transformer((enc_onehot, cat_onehot), \n",
    "                                       (enc_ordinal, cat_ordinal),\n",
    "                                       remainder='passthrough') # MinMaxScaler()\n",
    "\n",
    "#The make_column_transformer function automates transformations to featurs. \n",
    "# It transforms the featurs sequentially according to the order we will give into this function.\n",
    "\n",
    "# (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_onehot) \n",
    "# handle_unknown = \"ignore\" parameter transforms all the featurs in the cat_onehot list, \n",
    "# converting all categorical data that pass in the test set and not in the train set to 0.\n",
    "\n",
    "# (OrdinalEncoder(categories=categories), cat_ordinal) \n",
    "#After the onehotencoder conversion to the relevant featurs, all the featurs in the cat_ordinal list \n",
    "# perform ordinal encoder conversion according to the hierarchical order of the unique categorical observations\n",
    "# in the categories list.\n",
    "\n",
    "# remainder='passthrough' means leave all other unconverted features as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Ridge\", Ridge())]\n",
    "\n",
    "ridge_pipe = Pipeline(steps=operations)\n",
    "\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "# pipe_model.fit(X,y) makes\n",
    "# Since the first operation in pipe_model is column_trans;\n",
    "# 1. Onehotencoder conversion is done to the features (cat_onehot) in the X data that will be transformed into onehotencoder.\n",
    "# 2. The features (cat_ordinal) in the X data to which the ordinalencoder transformation will be applied are converted to ordinalencoder.\n",
    "# 3. Except for the converted featurs, no action is taken on the remaining featurs, and they are left as they are.\n",
    "# The second action in pipe_model is MinMaxScaler();\n",
    "# 4. After the transformation, minmax scale is applied to the new numeric X that we obtained. \n",
    "# Since there are dummies consisting of 0 and 1 in our data, minmaxscaler was applied so that these observations \n",
    "# remain 0 and 1 again.\n",
    "# The third action in pipe_model is Lasso();\n",
    "# The training is completed by giving y Lasso to the model along with the transformed and scaled X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2ROwYsymqZO"
   },
   "source": [
    "## Implement Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RvdF8CRmqZO"
   },
   "source": [
    "- Import the modul \n",
    "- Do not forget to scale the data or use Normalize parameter as True \n",
    "- Fit the model \n",
    "- Predict the test set \n",
    "- Evaluate model performance (use performance metrics for regression) \n",
    "- Tune alpha hiperparameter by using [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and determine the optimal alpha value.\n",
    "- Fit the model and predict again with the new alpha value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Ridge\", Ridge())]\n",
    "\n",
    "ridge_model = Pipeline(steps=operations)\n",
    "\n",
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(ridge_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Ridge\", Ridge())]\n",
    "pipe_model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(pipe_model, X_train, y_train,\n",
    "                        scoring=['r2', 'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], \n",
    "                        cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best alpha for Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_space = np.linspace(0.01, 100, 100)\n",
    "alpha_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Ridge\", Ridge())]\n",
    "pipe_model = Pipeline(steps=operations)\n",
    "\n",
    "param_grid = {'Ridge__alpha':alpha_space}  # Parameter names should be used with the model name defined in the pipeline.\n",
    "\n",
    "ridge_grid_model = GridSearchCV(estimator=pipe_model,\n",
    "                          param_grid=param_grid,\n",
    "                          scoring='neg_root_mean_squared_error',\n",
    "                          cv=10,\n",
    "                          n_jobs = -1,\n",
    "                          return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.get_params()  #To see the parameters of the model defined with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ridge_grid_model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid_model.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ridge_grid_model.cv_results_).loc[1, [\"mean_test_score\", \"mean_train_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(ridge_grid_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge_grid_model.predict(X_test)\n",
    "rm_R2 = r2_score(y_test, y_pred)\n",
    "rm_mae = mean_absolute_error(y_test, y_pred)\n",
    "rm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Ridge\", Ridge(alpha=1.02))]\n",
    "\n",
    "ridge_model = Pipeline(steps=operations)\n",
    "\n",
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model[\"Ridge\"].coef_  # In order to get the coefficients, the model name you know through the model \n",
    "                            # created with the pipeline should be used as a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model[\"OneHot_Ordinal_Encoder\"].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ridge_model[\"Ridge\"].coef_, index = ridge_model[\"OneHot_Ordinal_Encoder\"].get_feature_names_out(), columns=[\"Coef\"]).sort_values(\"Coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDfXOSnpmqZO"
   },
   "source": [
    "## 5. Implement Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icqyxzBymqZO"
   },
   "source": [
    "- Import the modul \n",
    "- Do not forget to scale the data or use Normalize parameter as True(If needed)\n",
    "- Fit the model \n",
    "- Predict the test set \n",
    "- Evaluate model performance (use performance metrics for regression) \n",
    "- Tune alpha hyperparameter by using [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and determine the optimal alpha value.\n",
    "- Fit the model and predict again with the new alpha value.\n",
    "- Compare different evaluation metrics\n",
    "\n",
    "*Note: To understand the importance of the alpha hyperparameter, you can observe the effects of different alpha values on feature coefficants.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbGqu0u5mqZO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso())]\n",
    "\n",
    "lasso_model = Pipeline(steps=operations)\n",
    "\n",
    "lasso_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(lasso_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso())]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "scores = cross_validate(model, X_train, y_train,\n",
    "                        scoring=['r2', 'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'],\n",
    "                        cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best alpha for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso())]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "param_grid = {'Lasso__alpha':alpha_space}  # Parameter names should be used with the model name defined in the pipeline.\n",
    "\n",
    "lasso_grid_model = GridSearchCV(estimator=model,\n",
    "                          param_grid=param_grid,\n",
    "                          scoring='neg_root_mean_squared_error',\n",
    "                          cv=10,\n",
    "                          n_jobs = -1,\n",
    "                          return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lasso_grid_model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid_model.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lasso_grid_model.cv_results_).loc[1, [\"mean_test_score\", \"mean_train_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(lasso_grid_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso_grid_model.predict(X_test)\n",
    "lasm_R2 = r2_score(y_test, y_pred)\n",
    "lasm_mae = mean_absolute_error(y_test, y_pred)\n",
    "lasm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso(alpha=1.02))]\n",
    "\n",
    "lasso_model = Pipeline(steps=operations)\n",
    "\n",
    "lasso_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lasso_model[\"Lasso\"].coef_, index = lasso_model[\"OneHot_Ordinal_Encoder\"].get_feature_names_out(), columns=[\"Coef\"]).sort_values(\"Coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpoOypMbmqZO"
   },
   "source": [
    "## 6. Implement Elastic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El7SNJUemqZO"
   },
   "source": [
    "- Import the modul \n",
    "- Do not forget to scale the data or use Normalize parameter as True(If needed)\n",
    "- Fit the model \n",
    "- Predict the test set \n",
    "- Evaluate model performance (use performance metrics for regression) \n",
    "- Tune alpha hyperparameter by using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and determine the optimal alpha value.\n",
    "- Fit the model and predict again with the new alpha value.\n",
    "- Compare different evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKVr6v2JmqZO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"ElasticNet\", ElasticNet())]\n",
    "\n",
    "elastic_model = Pipeline(steps=operations)\n",
    "\n",
    "elastic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(elastic_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"ElasticNet\", ElasticNet())]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(model, X_train, y_train,\n",
    "                        scoring=['r2', 'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], \n",
    "                        cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best alpha and l1_ratio for ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"ElasticNet\", ElasticNet())]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "param_grid = {'ElasticNet__alpha':[1.02, 2,  3, 4, 5, 7, 10, 11],\n",
    "              'ElasticNet__l1_ratio':[.5, .7, .9, .95, .99, 1]}\n",
    "\n",
    "elastic_grid_model = GridSearchCV(estimator=model,\n",
    "                          param_grid=param_grid,\n",
    "                          scoring='neg_root_mean_squared_error',\n",
    "                          cv=10,\n",
    "                          n_jobs = -1,\n",
    "                          return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid_model.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(elastic_grid_model.cv_results_).loc[5, [\"mean_test_score\", \"mean_train_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(elastic_grid_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = elastic_grid_model.predict(X_test)\n",
    "em_R2 = r2_score(y_test, y_pred)\n",
    "em_mae = mean_absolute_error(y_test, y_pred)\n",
    "em_rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature İmportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso(alpha=1.02))]\n",
    "model = Pipeline(steps=operations)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_imp = pd.DataFrame(model[\"Lasso\"].coef_, index = model[\"OneHot_Ordinal_Encoder\"].get_feature_names_out(), columns=[\"Coef\"]).sort_values(\"Coef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,14))\n",
    "sns.barplot(data= df_feat_imp, x=df_feat_imp.Coef, y=df_feat_imp.index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cannot view feature importance with yellowbrick when using pipeline.\n",
    "\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from yellowbrick.features import RadViz\n",
    "\n",
    "X_train_trans= column_trans.fit_transform(X_train)\n",
    "X_train_scaled= scaler.fit_transform(X_train_trans)\n",
    "model = Lasso(alpha=1.02)\n",
    "\n",
    "viz = FeatureImportances(model, labels=column_trans.get_feature_names_out())\n",
    "visualizer = RadViz(size=(720, 3000))\n",
    "viz.fit(X_train_scaled, y_train)\n",
    "viz.show();\n",
    "\n",
    "# We do not forget that we need to use the lasso model, as we will do feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df2[[\"make_model\", \"hp_kW\", \"km\",\"age\", \"Gearing_Type\", \"Gears\", \"Type\", 'Safety_Security_Package', \"price\"]]\n",
    "\n",
    "# We choose the top 7 features that have the most impact on the prediction. Here, a question may come up as why \n",
    "# the make_model feature was chosen. When the above image is examined, the make_model feature is among the features\n",
    "# that have the most impact on estimation.Since we saw that it has unique \n",
    "# categorical observations (Audi A3, AudiA1, Renault Espace etc.), we chose the make_model feature.\n",
    "\n",
    "# Although the 'Safety_Security_Package' featura doesn't contribute much to the estimation, \n",
    "# it is chosen to show how ordinalencoder conversions automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_new.drop(columns = [\"price\"])\n",
    "y = df_new.price\n",
    "\n",
    "# According to our new 7-featured dataset, we determine our X and y and reconstruct the model. \n",
    "# And we repeat the operations we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_onehot = ['make_model', 'Type', 'Gearing_Type']\n",
    "cat_ordinal = ['Safety_Security_Package']\n",
    "\n",
    "Safety_Security_Package = ['Safety Standard Package', 'Safety Premium Package', 'Safety Premium Plus Package']\n",
    "    \n",
    "categories = [Safety_Security_Package]\n",
    "\n",
    "column_trans = make_column_transformer((OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_onehot), \n",
    "                                       (OrdinalEncoder(categories=categories), cat_ordinal),\n",
    "                                       remainder='passthrough') #MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso(alpha=1.02))]\n",
    "lasso_final_model = Pipeline(steps=operations)\n",
    "\n",
    "lasso_final_model.fit(X_train, y_train)\n",
    "train_val(lasso_final_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso(alpha=1.02))]\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(model, X_train, y_train,\n",
    "                        scoring=['r2', 'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'],\n",
    "                        cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores, index = range(1, 11))\n",
    "scores.iloc[:,2:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2303/df_new.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso_final_model.predict(X_test)\n",
    "fm_R2 = r2_score(y_test, y_pred)\n",
    "fm_mae = mean_absolute_error(y_test, y_pred)\n",
    "fm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYyuZQtWmqZO"
   },
   "source": [
    "## 7. Visually Compare Models Performance In a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\"linear_m\": {\"r2_score\": lm_R2 , \n",
    " \"mae\": lm_mae, \n",
    " \"rmse\": lm_rmse},\n",
    "\n",
    " \"ridge_m\": {\"r2_score\": rm_R2, \n",
    " \"mae\": rm_mae,\n",
    " \"rmse\": rm_rmse},\n",
    "    \n",
    " \"lasso_m\": {\"r2_score\": lasm_R2, \n",
    " \"mae\": lasm_mae, \n",
    " \"rmse\": lasm_rmse},\n",
    "\n",
    " \"elastic_m\": {\"r2_score\": em_R2, \n",
    " \"mae\": em_mae, \n",
    " \"rmse\": em_rmse},\n",
    "         \n",
    " \"final_m\": {\"r2_score\": fm_R2, \n",
    " \"mae\": fm_mae , \n",
    " \"rmse\": fm_rmse}}\n",
    "scores = pd.DataFrame(scores).T\n",
    "scores\n",
    "\n",
    "# We assign the metrics we obtained from all models to the scores variable in jason format. Later to see model names\n",
    "# In the index and metrics in the features we take the transpose of the df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = scores.sort_values(by=\"r2_score\", ascending=False)\n",
    "compare\n",
    "#sns.barplot(x = compare[j] , y= compare.index)\n",
    "\n",
    "# We reorder the compare df by r2_scores from largest to smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = scores.columns\n",
    "\n",
    "for i, j in enumerate(scores):\n",
    "    plt.figure(i)\n",
    "    if j == \"r2_score\":\n",
    "        ascending = False # if our metric is r2_score the barplot will be sorted from largest to smallest\n",
    "    else:\n",
    "        ascending = True # if our metric is mae or rmse then the barplot will be sorted from smallest to largest\n",
    "    compare = scores.sort_values(by=j, ascending=ascending) # Reordering compare df by relevant metric\n",
    "    ax = sns.barplot(x = compare[j] , y= compare.index) # Metric scores for compare[j] from compare df are drawn \n",
    "                                                       # sequentially and visualized on the barplot.\n",
    "                                                    # y=compare.index will write the model names on the y-axis of our image.\n",
    "    ax.bar_label(ax.containers[0], fmt=\"%.4f\"); # The annotate is arranged as 4 digits from \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_new.drop(columns = [\"price\"])\n",
    "y = df_new.price\n",
    "\n",
    "# After trying all models and deciding on the model with the most optimal score, we separate the data \n",
    "# we use for this model as X and y. Note that we do not train and test split in the final stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"OneHot_Ordinal_Encoder\", column_trans), (\"scaler\", MinMaxScaler()), (\"Lasso\", Lasso(alpha=1.02))]\n",
    "final_model = Pipeline(steps=operations)\n",
    "\n",
    "# We set up the model with the best hyper parameter we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    \"make_model\": 'Audi A3',\n",
    "    \"hp_kW\": 66,\n",
    "    \"km\": 17000,\n",
    "    \"age\": 2,\n",
    "    \"Gearing_Type\": \"Automatic\",\n",
    "    \"Gears\": 7,\n",
    "    \"Type\":\"Used\",\n",
    "    'Safety_Security_Package':'Safety Premium Package'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "\n",
    "# The feature order of the observation we will predict should be the same as the feature order of the data we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = pd.DataFrame([my_dict])\n",
    "new_obs\n",
    "\n",
    "# We saw that the feature rankings are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.predict(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when feature order is different\n",
    "\n",
    "my_dict = {\n",
    "    \"make_model\": 'Audi A3',\n",
    "    \"km\": 17000,\n",
    "    \"hp_kW\": 66,\n",
    "    \"age\": 2,\n",
    "    \"Gearing_Type\": \"Automatic\",\n",
    "    \"Gears\": 7,\n",
    "    \"Type\":\"Used\",\n",
    "    'Safety_Security_Package':'Safety Premium Package'\n",
    "}\n",
    "\n",
    "new_obs = pd.DataFrame([my_dict])\n",
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.predict(new_obs)\n",
    "\n",
    "# The feature order of new_obs and X is different. make_column_transformer detects this difference and changes the feature order of new_obs.\n",
    "# makes it same for the feature order of the X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does pipe_model.predict(new_obs) do in order?\n",
    "# Since the first operation in pipe_model is column_trans;\n",
    "# 1. Onehotencoder conversion will be applied to new_obs data (cat_onehot) using X data\n",
    "# 2. Ordinalencoder conversion will be applied to new_obs data (cat_ordinal) using X data \n",
    "# 3. Remainder data in new_obs data left as they are\n",
    "# The second action in pipe_model is MinMaxScaler();\n",
    "# 4. The minmax scale is applied to the new numeric new_obs we get after the transformation, \n",
    "#    according to the min and max information of the X data. \n",
    "# The third action in pipe_model is Lasso();\n",
    "# 5. The transformed and scaled new_obs data using X's metrics is predicted by the Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: make_column_transformer function assigns the categorical featurs to the beginning of the df\n",
    "# and the numeric featurs to the end of the df in accordance with the transformation order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "modeling_auto_scout_Student_V1.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
